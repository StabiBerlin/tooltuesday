{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf1609-8a4a-43d1-876f-b92d7f91cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fafd75-073e-49cd-bb6b-43a12d894846",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-german\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-german\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2e2cc-10ea-41d0-abec-031295d81d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/fontane_brandenburg01_1862_ch1.txt\", 'r', encoding='utf-8') as f:\n",
    "    fontane=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514f3fe-5c51-44ec-94ee-6b45b1d4029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fontane))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc80381-efd4-4692-9dfc-f92436d547a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_results = nlp(fontane)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d6cab-5576-4548-a82b-ff96164759d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ner_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086990b7-c7b8-430f-abbc-f866bfb10cce",
   "metadata": {},
   "source": [
    "Text überschreitet Input-Länge des Modells. Chunking nötig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7353d4-42c3-48d8-aa89-414ba765990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_chunks(text, nlp_pipeline, chunk_chars=3000, overlap=100, batch_size=16):\n",
    "    assert 0 <= overlap < chunk_chars, \"overlap must be >=0 and < chunk_chars\"\n",
    "    step = chunk_chars - overlap\n",
    "    L = len(text)\n",
    "    starts = list(range(0, L, step))\n",
    "    chunks = [ text[s: min(s + chunk_chars, L)] for s in starts ]\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i : i + batch_size]\n",
    "        batch_out = nlp_pipeline(batch)\n",
    "        for j, ents in enumerate(batch_out):\n",
    "            chunk_start = starts[i + j]\n",
    "            for e in ents:\n",
    "                if 'start' in e and 'end' in e:\n",
    "                    e2 = dict(e)\n",
    "                    e2['start'] += chunk_start\n",
    "                    e2['end'] += chunk_start\n",
    "                    results.append(e2)\n",
    "\n",
    "    # dedupe\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for e in sorted(results, key=lambda x: (x['start'], x['end'])):\n",
    "        key = (e.get('start'), e.get('end'), e.get('entity_group') or e.get('entity'), e.get('word'))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(e)\n",
    "    return uniq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f88d2f-a35a-4681-a74c-fe0db60f8c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = ner_chunks(fontane, nlp, chunk_chars=1200, overlap=50)\n",
    "print(len(ents), ents[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5231f7ce-4a5c-4699-9941-cb49ecfd4575",
   "metadata": {},
   "source": [
    "Spacy (displacy) nutzen, um die Ergebnisse darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27929337-e77d-4320-ba15-a28c8a493f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_spacy_doc(text, entities, lang=\"de\"):\n",
    "    \"\"\"\n",
    "    Convert the raw text + entities (with absolute 'start'/'end') into a spaCy Doc.\n",
    "    Entities must have: start, end, entity_group/entity.\n",
    "    \"\"\"\n",
    "    # 1) Create blank spaCy model (does NOT tokenize automatically into words)\n",
    "    nlp = spacy.blank(lang)\n",
    "    \n",
    "    # 2) Create doc as one continuous text; spaCy will tokenize it\n",
    "    doc = nlp(text)\n",
    "\n",
    "    spans = []\n",
    "    for e in entities:\n",
    "        start = e[\"start\"]\n",
    "        end = e[\"end\"]\n",
    "        label = e.get(\"entity_group\") or e.get(\"entity\")\n",
    "\n",
    "        # spaCy requires token-aligned spans — so we use char span with alignment mode 'contract'\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "\n",
    "        if span is None:\n",
    "            # If a token boundary misalignment happens, skip it gracefully\n",
    "            # (usually rare unless tokenizer splits oddly)\n",
    "            continue\n",
    "\n",
    "        spans.append(span)\n",
    "\n",
    "    # assign entities to doc\n",
    "    doc.set_ents(spans, default=\"unmodified\")\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db34d1b-aba9-4c79-99d5-12e5879638cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = results_to_spacy_doc(fontane, ents, lang=\"de\")\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
